# -*- coding: utf-8 -*-
"""House_Price_Prediction_Using_Ames_Housing_Data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wSZln7FAhP1QiONqq0ImV_5xTI0QXdPk
"""

# Imports
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import os
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import metrics
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.impute import SimpleImputer

pd.options.display.float_format = '{:.4f}'.format # scientific notation
pd.set_option('display.max_columns', None) # see cols

# To check my working directory
os.getcwd()

#load both training and testing datasets
tr = pd.read_csv('train.csv')
ts = pd.read_csv('test.csv')

ts

all_data = pd.concat([tr, ts], sort=False)

#change column name to lower case
tr.columns = tr.columns.str.lower()
ts.columns = ts.columns.str.lower()
all_data.columns = all_data.columns.str.lower()



#check for the first 2 rows in the testing dataset
ts.head(2)

all_data.head(2)

tr.describe()

#check for missing data
tr.info()

tr['fireplace_qu'].unique()

tr.shape

all_data.shape

#To define the scale
ordinal_mapping = {
    np.nan: 0,
    'TA': 1,
    'Fa': 2,
    'Gd': 3,
    'Po': 4,
    'Ex': 5,
}

# map the scale to the fireplace column
tr['fireplace_qu']=tr['fireplace_qu'].map(ordinal_mapping)
ts['fireplace_qu']=ts['fireplace_qu'].map(ordinal_mapping)
all_data['fireplace_qu']=all_data['fireplace_qu'].map(ordinal_mapping)

# map the scale to the Exterior material quality column
tr['exter_qual']=tr['exter_qual'].map(ordinal_mapping)
ts['exter_qual']=ts['exter_qual'].map(ordinal_mapping)
all_data['exter_qual']=all_data['exter_qual'].map(ordinal_mapping)

# map the scale to the Exterior material quality column
tr['exter_qual']=tr['exter_qual'].map(ordinal_mapping)
ts['exter_qual']=ts['exter_qual'].map(ordinal_mapping)
all_data['exter_qual']=all_data['exter_qual'].map(ordinal_mapping)

# map the scale to the Present condition of the material on the exterior
tr['exter_cond']=tr['exter_cond'].map(ordinal_mapping)
ts['exter_cond']=ts['exter_cond'].map(ordinal_mapping)
all_data['exter_cond']=all_data['exter_cond'].map(ordinal_mapping)

tr['bsmt_qual']=tr['bsmt_qual'].map(ordinal_mapping)
ts['bsmt_qual']=ts['bsmt_qual'].map(ordinal_mapping)
all_data['bsmt_qual']=all_data['bsmt_qual'].map(ordinal_mapping)

tr['bsmt_cond']=tr['bsmt_cond'].map(ordinal_mapping)
ts['bsmt_cond']=ts['bsmt_cond'].map(ordinal_mapping)
all_data['bsmt_cond']=all_data['bsmt_cond'].map(ordinal_mapping)

tr['kitchen_qual']=tr['kitchen_qual'].map(ordinal_mapping)
ts['kitchen_qual']=ts['kitchen_qual'].map(ordinal_mapping)
all_data['kitchen_qual']=all_data['kitchen_qual'].map(ordinal_mapping)


tr['heating_qc']=tr['heating_qc'].map(ordinal_mapping)
ts['heating_qc']=ts['heating_qc'].map(ordinal_mapping)
all_data['heating_qc']=all_data['heating_qc'].map(ordinal_mapping)


tr['garage_qual']=tr['garage_qual'].map(ordinal_mapping)
ts['garage_qual']=ts['garage_qual'].map(ordinal_mapping)
all_data['garage_qual']=all_data['garage_qual'].map(ordinal_mapping)

tr['garage_cond']=tr['garage_cond'].map(ordinal_mapping)
ts['garage_cond']=ts['garage_cond'].map(ordinal_mapping)
all_data['garage_cond']=all_data['garage_cond'].map(ordinal_mapping)

tr['pool_qc']=tr['pool_qc'].map(ordinal_mapping)
ts['pool_qc']=ts['pool_qc'].map(ordinal_mapping)
all_data['pool_qc']=all_data['pool_qc'].map(ordinal_mapping)

#To define the scale
ordinal_mapping_2 = {
    np.nan: 0,
    'no': 0,
    'No': 0,
    'NO': 0,
    'n': 0,
    'N': 0,
    'yes': 1,
    'Yes': 1,
    'YES': 1,
    'y': 1,
    'Y': 1 }

# map the scale to the central air
tr['central_air']=tr['central_air'].map(ordinal_mapping)
ts['central_air']=ts['central_air'].map(ordinal_mapping)
all_data['central_air']=all_data['central_air'].map(ordinal_mapping)

tr['paved_drive']=tr['paved_drive'].map(ordinal_mapping)
ts['paved_drive']=ts['paved_drive'].map(ordinal_mapping)

all_data['paved_drive']=all_data['paved_drive'].map(ordinal_mapping)

tr.head()

all_data.head()

tr.rename(columns = {'1st_flr_sf':'ft_flr_sf','2nd_flr_sf':'sd_flr_sf'}, inplace = True)
ts.rename(columns = {'1st_flr_sf':'ft_flr_sf','2nd_flr_sf':'sd_flr_sf'}, inplace = True)
all_data.rename(columns = {'1st_flr_sf':'ft_flr_sf','2nd_flr_sf':'sd_flr_sf'}, inplace = True)

tr.rename(columns = {'3ssn_porch':'td_ssn_porch'}, inplace = True)
ts.rename(columns = {'3ssn_porch':'td_ssn_porch'}, inplace = True)
all_data.rename(columns = {'3ssn_porch':'td_ssn_porch'}, inplace = True)

tr.isna().sum()

# Identify string columns with NaN values
str_tr_col = [colm for colm in tr.select_dtypes(include='object') if tr[colm].isnull().any()]
str_ts_col = [colm for colm in tr.select_dtypes(include='object') if tr[colm].isnull().any()]
str_al_d_col = [colm for colm in tr.select_dtypes(include='object') if tr[colm].isnull().any()]

# Drop these columns
tr.drop(columns=str_tr_col, inplace=True)
ts.drop(columns=str_ts_col, inplace=True)
all_data.drop(columns=str_al_d_col, inplace=True)

all_data.head(2)

tr.columns

#check for columns with integers and plot the heatmap
int_columns_tr = tr.select_dtypes(include=['int', 'int64', 'int32']).columns
plt.figure(figsize = (16,12))
corr_matrix = tr[int_columns_tr].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')

#Shuffling my best variables
col = [
        #'id'
       #, 'pid'
       #,
        #'ms_subclass'
       #,
    'lot_area'
       , 'overall_qual'
       #, 'overall_cond'
       ,'year_built'
       , 'year_remod_add'
       #, 'exter_qual'
       , 'bsmt_cond'
       #, 'bsmt_cond'
       , 'heating_qc'
       #,'central_air'
       , 'ft_flr_sf'
       , 'sd_flr_sf'
       #, 'low_qual_fin_sf'
       ,'gr_liv_area'
       , 'full_bath'
       , 'half_bath'
       , 'bedroom_abvgr'
       #,'kitchen_abvgr'
       , 'kitchen_qual'
       , 'totrms_abvgrd'
       , 'fireplaces'
       #,'garage_qual'
       #, 'garage_cond'
       , 'wood_deck_sf'
       , 'open_porch_sf'
       #,'enclosed_porch'
       #, 'td_ssn_porch'
       , 'screen_porch'
       , 'pool_area'
       ,'pool_qc'
       #, 'misc_val'
       #, 'mo_sold'
       , 'yr_sold'
      #'saleprice'
      ]

X = tr[col]
plt.figure(figsize = (20,12))
corr_matrix = X.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')

"""# Build a baseline model"""

baseline_predictions = [y_train.mean()]*len(y_val)
bp = baseline_predictions.copy()

pd.Series(bp).head()

#baseline MAE
bp_mae = mean_absolute_error(y_val, bp)
bp_mae

#baseline MSE
bp_mse = mean_squared_error(y_val, bp)
bp_mse

bp_rmse = mean_squared_error(y_val, bp, squared = False)
bp_rmse

bp_rsc = r2_score(y_val, bp)
bp_rsc

"""# Now, let's build a LinearRegression model for our predictions.

"""

#Assign data to X and y
X = tr[col]
y  = tr['saleprice']
X_test = ts[col]

#Split my train.csv data into training and testing data
X_train, X_val, y_train, y_val = train_test_split(X, y, random_state = 2024)

X_train.shape

X_val.shape

y_train.shape

"""# Preprosessing"""

#Simple Inputer
si = SimpleImputer(strategy='mean')
X_train_si = si.fit_transform(X_train)
X_val_si = si.transform(X_val)

# Applying standardScaling
sc = StandardScaler()
X_train_sc = sc.fit_transform(X_train_si)
X_val_sc = sc.transform(X_val_si)

# Instantiate and fit Linear Regression model
lr = LinearRegression()
lr.fit(X_train_sc, y_train)

lr.score(X_train_sc, y_train)

lr.score(X_val_sc, y_val)

"""### The r^2 score for the train data is 82.31% and the r^2 for the validation data is 78.4%. This shows that the model is good enough."""

y_val_pred = lr.predict(X_val_sc)
y_val_pred

residuals = y_val - y_val_pred
residuals

# Plotting the residuals
plt.figure(figsize=(10, 6))
plt.scatter(y_val, residuals)
plt.xlabel('Actual Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')

# Plotting the predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_val, y_val_pred, label='Linear Regression')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values')

#To find the avaluation metrics
lr_mae = mean_absolute_error(y_val, y_val_pred)
lr_rmse = np.sqrt(lr_mse)
lr_rsc = r2_score(y_val, y_val_pred)
lr_mse = mean_squared_error(y_val, y_val_pred)
print ('mean absolut:', lr_mae)
print ('mean squ:',lr_mse)
print ('root mean sq:',lr_rmse)
print ('r score:',lr_rsc)

# Plotting the baseline predictions
plt.scatter(y_val, bp, color='blue', label='Baseline Model')

# Plotting the linear regression model predictions
plt.scatter(y_val, y_val_pred, color='red', label='Linear Regression Model')
plt.plot(np.unique(y_val), np.poly1d(np.polyfit(y_val, y_val_pred, 1))(np.unique(y_val)), color='green', label='Line of Best Fit')


# Adding labels and legend
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.legend()

# Adding a title
plt.title('Comparison of Baseline Model and Linear Regression Model')





# Generate predictions on the test set
test_predictions = lr.predict(X_val_sc)

# Optionally, save the predictions to a CSV file
output = pd.DataFrame({'Id':ts.id, 'SalePrice': test_predictions})
output.to_csv('test_predictions.csv', index=False)